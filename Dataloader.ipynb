{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from Dataset import TrashDataset\n",
    "from pathlib import Path\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "train_transforms = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.Resize((224,224)),\n",
    "    v2.RandomVerticalFlip(0.5)\n",
    "])\n",
    "\n",
    "classes=['c_1','c_2_01','c_2_02','c_3',\n",
    "         'c_4_01_02','c_4_02_01_02',\n",
    "         'c_4_02_02_02','c_4_02_03_02','c_4_03','c_5_02',\n",
    "         'c_6','c_7','c_1_01','c_2_02_01',\n",
    "         'c_3_01','c_4_03_01','c_5_01_01',\n",
    "         'c_5_02_01','c_6_01','c_7_01',\n",
    "         'c_4_01_01','c_4_02_01_01',\n",
    "         'c_4_02_02_01','c_4_02_03_01',\n",
    "         'c_5_01','c_8_01','c_8_02',\n",
    "         'c_8_01_01','c_9']\n",
    "\n",
    "def od_collate_fn(batch):\n",
    "    '''\n",
    "    Dataset에서 꺼내는 어노테이션 데이터의 크기는 화상마다 다름.\n",
    "    화상 내의 물체 수가 두개이면 (2,5) 사이즈 [xmin, ymin, xmax, ymax, label]이 2개, \n",
    "    물체 수가 3개 이면 (3,5) 사이즈 [Xmin, Xmax, ymin, ymax, label]가 3개\n",
    "    변화에 대응하는 DataLoader를 만드는 collate_fn을 작성\n",
    "    collate_fn은 파이토치 리스트로 mini_batch를 작성하는 함수이다.\n",
    "    '''\n",
    "    targets=[]\n",
    "    images=[]\n",
    "    # batch = transformed_image, bbox , label\n",
    "    for transformed_image, bbox, label in batch:\n",
    "        images.append(transformed_image/255)\n",
    "        targets.append({'boxes':torch.tensor(bbox).to('cuda'), 'labels':torch.tensor(label).to('cuda')})\n",
    "    \n",
    "    return images,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images : [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]\n",
      "targets: [{'boxes': tensor([[162,   0, 193,  73]], device='cuda:0'), 'labels': tensor([3], device='cuda:0')}, {'boxes': tensor([[ 88,  84, 126, 157],\n",
      "        [106, 120, 154, 172],\n",
      "        [ 34,  72,  55, 114],\n",
      "        [ 85, 169, 110, 216],\n",
      "        [ 98,   0, 130,  60],\n",
      "        [158,   0, 174,  24],\n",
      "        [174,  41, 182,  80]], device='cuda:0'), 'labels': tensor([ 3,  3, 10, 18, 10, 10, 10], device='cuda:0')}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyssk\\MyThesisProject\\Dataset.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  transformed_img, transformed_boxes = self.transform(torch.tensor(img),boxes)\n",
      "c:\\Users\\hyssk\\anaconda3\\envs\\objectdetection\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "C:\\Users\\hyssk\\AppData\\Local\\Temp\\ipykernel_29612\\3145899538.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets.append({'boxes':torch.tensor(bbox).to('cuda'), 'labels':torch.tensor(label).to('cuda')})\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_anno_path = Path(r'C:\\Users\\hyssk\\MyThesisProject\\307.생활폐기물 데이터 활용ㆍ환류\\01-1.정식개방데이터\\Training\\02.라벨링데이터')\n",
    "    train_img_dir = Path(r'C:\\Users\\hyssk\\MyThesisProject\\307.생활폐기물 데이터 활용ㆍ환류\\01-1.정식개방데이터\\Training\\01.원천데이터')\n",
    "\n",
    "    test_img_dir= Path(r'C:\\Users\\hyssk\\MyThesisProject\\307.생활폐기물 데이터 활용ㆍ환류\\01-1.정식개방데이터\\Validation\\01.원천데이터')\n",
    "    test_anno_path= Path(r'C:\\Users\\hyssk\\MyThesisProject\\307.생활폐기물 데이터 활용ㆍ환류\\01-1.정식개방데이터\\Validation\\02.라벨링데이터')\n",
    "\n",
    "    train_dataset = TrashDataset(classes,train_anno_path,train_img_dir,train_transforms)\n",
    "    train_dataloader = DataLoader(train_dataset,2,True,collate_fn=od_collate_fn)\n",
    "    images, targets = next(iter(train_dataloader))\n",
    "    print(f'images : {images}')\n",
    "    print(f'targets: {targets}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "objectdetection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
